# llm_client.py

import os
import requests
from google import genai
from langchain_community.embeddings import SentenceTransformerEmbeddings
import streamlit as st

# --- Configuraci√≥n de Modelos ---
EMBEDDING_MODEL_LOCAL = "all-MiniLM-L6-v2"
LLM_MODEL = "gemini-2.0-flash"

CALENDAR_API_URL = os.getenv("CALENDAR_API_URL", "http://127.0.0.1:8000")


def get_gemini_client():
    """
    Inicializa y retorna el cliente directo de Gemini (google.genai).
    La clave GEMINI_API_KEY se toma autom√°ticamente del entorno.
    """
    try:
        if not os.getenv("GEMINI_API_KEY"):
            raise ValueError("GEMINI_API_KEY no est√° configurada.")
        return genai.Client()
    except Exception as e:
        # Re-lanzamos la excepci√≥n para que Streamlit la muestre
        raise Exception(f"Error al inicializar cliente Gemini: {e}")


def get_local_embedding_function():
    """
    Inicializa y retorna la funci√≥n de embeddings de Sentence Transformers (local).
    """
    return SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL_LOCAL)


from rag import get_vector_store, get_rag_context


def _prepare_tools() -> list[genai.types.Tool]:
    """
    Descarga el esquema OpenAPI de la API Tool y lo usa para configurar la Tool.
    """
    tools = []
    api_uri = f"{CALENDAR_API_URL}/openapi.json"

    try:
        st.info(f"‚¨áÔ∏è Intentando descargar esquema OpenAPI desde: {api_uri}")

        # DESCARGAR el esquema OpenAPI (JSON)
        response = requests.get(api_uri)
        response.raise_for_status()  # Lanza una excepci√≥n para c√≥digos 4xx/5xx
        openapi_spec = response.json()

        # EXTRAER la declaraci√≥n de funci√≥n requerida por Gemini
        # FastAPI genera la especificaci√≥n OpenAPI. Debemos adaptarla a Tool.
        # Buscamos la especificaci√≥n de la √∫nica funci√≥n /calendar/query

        # El nombre de la funci√≥n en la especificaci√≥n de FastAPI debe ser √∫nico.
        # Lo m√°s f√°cil es extraer las declaraciones de funci√≥n del componente 'paths'.        
        # Usamos una forma manual de construir la Tool a partir de la especificaci√≥n
        # ya que genai.types.Tool.from_dict/from_json puede variar entre versiones.

        # Aqu√≠ asumiremos que conocemos la estructura de la funci√≥n 'query_f1_calendar'
        # que es la √∫nica en tu API Tool:

        # 1. Definir el diccionario del esquema de par√°metros
        parameters_dict = {
            'type': 'object',
            'properties': {
                p['name']: {
                    'type': p.get('schema', {}).get('type', 'string'),
                    'description': p.get('description', '')
                } 
                for p in openapi_spec['paths']['/calendar/query']['get']['parameters']
            }
        }
        # 2. Construir la FunctionDeclaration
        function_declaration = genai.types.FunctionDeclaration(
            name='query_f1_calendar',
            description=openapi_spec['paths']['/calendar/query']['get']['summary'],
            parameters=parameters_dict
        )

        calendar_tool = genai.types.Tool(function_declarations=[function_declaration])
        tools.append(calendar_tool)

        st.success(f"‚úÖ Tool configurada: **{function_declaration.name}** (Calendario)")

    except requests.exceptions.RequestException as e:
        st.error(f"‚ùå Error de red/HTTP al cargar la Tool desde {api_uri}. La API debe estar corriendo. Error: **{e}**")
    except Exception as e:
        st.error(f"‚ùå Error al parsear el esquema OpenAPI. Detalles: **{e}**")

    return tools


def _handle_function_call(client: genai.Client,
                          response_1: genai.types.GenerateContentResponse,                          
                          context_prompt: str) -> str:
    """
    Ejecuta la llamada a la funci√≥n (API Tool) y pasa el resultado al LLM.    
    """
    function_call = response_1.function_calls[0]
    function_name = function_call.name
    st.warning(f"ü§ñ El LLM ha decidido **ignorar el RAG y llamar a la Tool**: {function_name}")

    # 1. Construir URL de la API
    url_endpoint = "/calendar/query"
    params = function_call.args
    query_string = "&".join(f"{key}={value}" for key, value in params.items())
    full_url = f"{CALENDAR_API_URL}{url_endpoint}?{query_string}"

    st.code(f"üî® URL de la API generada:\n{full_url}", language="http")

    # 2. Ejecutar la llamada HTTP
    tool_output = None
    try:
        api_response = requests.get(full_url)
        api_response.raise_for_status()
        tool_output = api_response.json()
        # if tool output result is a list convert to dict
        if isinstance(tool_output, list):
            tool_output = {"calendar_entries": tool_output}
        st.success("‚úÖ Tool ejecutada exitosamente. Datos obtenidos.")

    except requests.exceptions.RequestException as e:
        st.error(f"‚ùå Error al llamar a la API Tool: {e}")
        # Aseguramos que tool_output sea un json dict para la segunda llamada
        tool_output = {"error": f"Fallo en la conexi√≥n/API: {e}"}

    # 3. Segunda Llamada a Gemini (Generaci√≥n Final de la Respuesta)

    # 3.1 Build content history for the second call
    contents_for_second_call = [
        # prompt original del usuario (context_prompt)
        genai.types.Content(
            role="user",
            parts=[genai.types.Part(text=context_prompt)]
        ),
        # reutilizamos el objeto Content original del modelo que contiene la FunctionCall
        response_1.candidates[0].content,
        # el resultado de la Tool (FunctionResponse)
        genai.types.Content(
            role="tool",
            parts=[genai.types.Part.from_function_response(
                name=function_name,
                response=tool_output
            )]
        )
    ]
    # 3.2 Execute second call
    second_response = client.models.generate_content(
        model=LLM_MODEL,
        contents=contents_for_second_call
    )
    return second_response.text


def unified_query_gemini(prompt: str) -> str:
    """
    Consulta central: configura RAG y Tools. El LLM decide qu√© recurso usar.
    """
    client = get_gemini_client()

    # 1. Preparar el contexto RAG
    vector_store = get_vector_store()
    rag_context_text, _ = get_rag_context(prompt, vector_store)

    # 1.1. Inyectar el contexto RAG en el prompt
    context_prompt = (
        f"CONTEXTO DE NOTICIAS RECIENTES (RAG):\n---\n{rag_context_text}\n---\n"
        f"Bas√°ndote en el contexto anterior o en la Tool API disponible, responde a la siguiente pregunta: {prompt}"
    )
    st.markdown("---")
    st.info(f"üîé Contexto RAG inyectado para la b√∫squeda de noticias.")

    # 2. Preparar la API Tool
    tools = _prepare_tools()

    # 3. Primera Llamada a Gemini (Decisi√≥n)
    response_1 = client.models.generate_content(
        model=LLM_MODEL,
        contents=[context_prompt],
        config=genai.types.GenerateContentConfig(
            tools=tools
        )
    )

    # 4. Manejo del Resultado
    if response_1.function_calls:
        # El LLM elige la Tool (Delegamos a la funci√≥n auxiliar)
        return _handle_function_call(client, response_1, context_prompt)
    else:
        # El LLM us√≥ el contexto RAG o su conocimiento interno
        st.success("üß† El LLM respondi√≥ usando el **Contexto RAG** o su conocimiento interno.")
        return response_1.text
